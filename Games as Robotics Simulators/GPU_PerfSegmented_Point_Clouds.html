<!DOCTYPE html>
<html>
<head>
<link rel="stylesheet" href="https://fonts.googleapis.com/icon?family=Material+Icons">
<link rel="stylesheet" href="https://code.getmdl.io/1.2.1/material.deep_orange-blue.min.css">
<script src="https://code.getmdl.io/1.2.1/material.min.js" defer></script><meta name="viewport" content="width=device-width, initial-scale=1.0">
<style>
.demo-ribbon {
  width: 100%;
  height: 40vh;
  //background-image: url("hero.png");
  background-color: #f5f5f5;
  flex-shrink: 0;
}

.demo-main {
  margin-top: -35vh;
  flex-shrink: 0;
}

.demo-header .mdl-layout__header-row {
  padding-left: 40px;
}

.demo-container {
  max-width: 1600px;
  width: calc(100% - 16px);
  margin: 0 auto;
}

.demo-content {
  border-radius: 2px;
  padding: 80px 56px;
  margin-bottom: 80px;
}

.demo-layout.is-small-screen .demo-content {
  padding: 40px 28px;
}

.demo-content h3 {
  margin-top: 48px;
}

.demo-footer {
  padding-left: 40px;
}

.demo-footer .mdl-mini-footer--link-list a {
  font-size: 13px;
}
#view-source {
    float: right;
}
</style>
<script type="text/x-mathjax-config">
  MathJax = {
    tex: {
      inlineMath: [['$', '$'], ["\(", "\)"]],
      processEscapes: true,
    }
  }
                    </script><script type="text/javascript" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js" for="1"></script><script type="text/javascript" src="../mermaid.min.js" for="1"></script><link rel="stylesheet" href="../colorful.css">
<link rel="stylesheet" href="../styles.css">
<link rel="stylesheet" href="../pygments.css">
<link rel="stylesheet" href="../colorful.css">
<script>
                    function show(toExpand)
                    {
                    if(
                       document.getElementById(toExpand).style.display == 'none'
                       ||
                       document.getElementById(toExpand).style.display == ''
                       )
                        document.getElementById(toExpand).style.display = 'block';
                    else
                        document.getElementById(toExpand).style.display = 'none';
                    }
                    </script><style></style>
</head>
<body><div class="demo-layout mdl-layout mdl-layout--fixed-header mdl-js-layout mdl-color--grey-100">
<header class="demo-header mdl-layout__header mdl-layout__header--scroll mdl-color--grey-100 mdl-color-text--grey-800"><div class="mdl-layout__header-row">
<span class="mdl-layout-title"><a href="../index.html" style="text-decoration:none; color:#444;" class="mdl-typography--headline">Tom Bertalan</a></span><div class="mdl-layout-spacer"></div>
</div></header><div class="demo-ribbon"></div>
<main class="demo-main mdl-layout__content"><div class="demo-container mdl-grid">
<div class="mdl-cell mdl-cell--2-col mdl-cell--hide-tablet mdl-cell--hide-phone"></div>
<div class="demo-content mdl-color--white mdl-shadow--4dp content mdl-color-text--grey-800 mdl-cell mdl-cell--8-col">
<div class="demo-crumbs mdl-color-text--grey-500">
<a href="../index.html">Home</a> &gt; <a href="index.html">Games as Robotics Simulators</a> &gt; <span>GPU Perf+Segmented Point Clouds</span>
</div>
<span></span><h1>GPU Perf+Segmented Point Clouds</h1>
<span></span><h5>Performance</h5>

<p>Went through the pipeline and made sure everything stays on the GPU until visualization time. Video <a href="https://youtu.be/LDE2x235soo">here</a>.</p>

<iframe width="560" height="315" src="https://www.youtube.com/embed/LDE2x235soo" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen></iframe>

<p>A few major contributors to the speed improvements since my first attempts (see <a href="https://www.youtube.com/playlist?list=PLK-JvNE4ewRrrJNaqkiZPIMvaOxRqJYrx">this video playlist</a>):</p>

<ul class="checklist">
<li>
<input type="checkbox" disabled checked> Use an image capture library that can keep data on GPU.</li>
<li>At first, I thought I'd have to use PyCuda and some opengl calls to do this myself, but now I <em>think</em> that <a href="https://pypi.org/project/d3dshot/">d3dshot is doing this if you ask for <code>output="pytorch_gpu"</code>,</a> or, even better (we need floats for the next step), <code>"pytorch_float_gpu"</code>.</li>
<li>
<a href="https://pypi.org/project/dxcam/">dxcam claims to offer "seamless integration with ... PyTorch"</a>, and has more recent releases than d3dshot, but, in practice, d3dshot was faster. :man_shrugging:</li>
<li>
<input type="checkbox" disabled> Still, dxcam further claims to be capable of 240 Hz, so maybe I was using it wrong.</li>
<li>
<input type="checkbox" disabled checked> Don't use Huggingface's provided pipelines, which will convert images to NumPy internally to do pre-processing like scaling. Instead, I recreated their preprocessing operations with torch operations on GPU ... as they actually <a href="https://huggingface.co/docs/transformers/tasks/monocular_depth_estimation#depth-estimation-inference-by-hand">suggest</a> (minus the GPU discussion).</li>
<li>
<input type="checkbox" disabled checked> Use some profiling to see what steps are really the slow ones! <img src="Screenshot%202023-05-16%20191128.png" style="height: 220px">
</li>
</ul>

<h5>Camera Parameters</h5>

<p>I then spent some time playing with camera parameters (focal lengths, sensor center definition, and extrinsic of rotation and translation) to backproject the depth data, colored by semantic segmentation, the camera's frame in physical 3D coordinates. (<a href="https://www.youtube.com/watch?v=8VyPjRhbJlo&amp;t=5200s">video</a>)</p>

<iframe width="560" height="315" src="https://www.youtube.com/embed/8VyPjRhbJlo" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen></iframe>

<p>For my reference, the relevant equations are</p>

<p>$\vec x = [\mathbf{R} | \vec T] \cdot \mathbf{K}^{-1} \cdot [\vec{u}; 1] d$</p>

<p>where</p>

<ol>
<li>$\vec x$ is the camera's 3d position;</li>
<li>the extrinsic parameters are</li>
<li>$\mathbf{R} = \left[ \begin{array}{ccc} c_z c_y,&amp; c_z s_y s_x - s_z c_x, &amp;c_z s_y c_x + s_z s_x \ s_z c_y,&amp;s_z s_y s_x + c_z c_x,&amp; s_z, s_y, c_x - c_z s_x \ -s_y,&amp;c_y s_x,&amp;c_y c_x \end{array} \right]$ , the $3\times3$  rotation defined by sines $s_i$ and cosines $c_i$ of the $i=x,y,z$ Euler angles,</li>
<li>and $\vec T$, the $3\times 1$ translation of the camera;</li>
<li>$\mathbf{K} = \left[ \begin{array}{ccc} \ell_u, &amp; 0, &amp; u_0 \ 0, &amp; \ell_v, &amp; v_0 \ 0,&amp;0,&amp;1 \end{array} \right]$ is the (intrinsic) camera matrix;</li>
<li>$\vec u$ is the $2\times1$ images space coordinates (defined as going from 0 to 1, from top left to bottom right);</li>
<li>and $d$ is the scalar depth (this equation is defined per pixel).</li>
</ol>

<p>So, I can precompute $[\mathbf{R} | \vec T] \cdot \mathbf{K}^{-1} \cdot [\vec{u}; 1]$ and, at runtime, just multiply pointwise by $d$, right?</p>

<p>Unfortunately, I'm a doofus, and I instead wrote</p>

<p>$\hat{\vec{T}} + \hat{\mathbf{R}}^{-1} \cdot \mathbf{K}^{-1} \cdot [\vec{u}; 1] d$</p>

<p>That is, I backproject, unrotate, and then translate instead of backproject, translate, and then rotate.</p>

<p>So, the best I can do is precompute $\hat{\mathbf{R}}^{-1} \cdot \mathbf{K}^{-1} \cdot [\vec{u}; 1]$ for now, until I</p>

<ul class="checklist">
<li>
<input type="checkbox" disabled> Redefine camera extrinsics so that translation comes before rotation in backprojection.</li>
</ul>

<p>With this definition, I tried setting up some scenes with visually identifyable geometry so I could twiddle the parameters until they looked sorta right. E.g., the <a href="https://www.tesla.com/ownersmanual/modelx/en_us/GUID-91E5877F-3CD2-4B3B-B2B8-B5DB4A6C0A05.html">Tesla model X</a> is about 5 meters long, 1.7 high, and 2 wide without mirrors:</p>

<p><img src="Screenshot%202023-05-16%20191214.png" alt="Screenshot 2023-05-16 191214" style="height: 220px"> <img src="Screenshot%202023-05-16%20191140.png" alt="Screenshot 2023-05-16 191140" style="height: 220px"></p>

<p>Definitions aside, one thing I found helpful was to realize that there's a relationship</p>

<p>$\ell_i = \frac{\Delta_i}{2 \tan (\alpha_i / 2)}$</p>

<p>between the focal length $\ell_i$ in direction $i=u,v$ and (1) the corresponding sensor dimension $\Delta_i$ and (2) field of view $\alpha_i$. Since $\alpha_i=\pi/3$ is one of the few things I actually do know from the game, I can enter this, and then adjust $\Delta_i$ instead of $\ell_i$, which seems to be less sensitive in hand-tuning.</p>

<p>However, I do think the next step here is to</p>

<ul class="checklist">
<li>
<input type="checkbox" disabled> Use <a href="https://colmap.github.io/cameras.html">COLMAP</a> to estimate camera parameters from a few frames, and for a few vehicles like <a href="https://github.com/cc959/Matura_Unreal">this guy says he did</a>.</li>
</ul>

<p>instead of eyeballing it. (This needs to be redone per vehicle since each requires a different crop of the whole view to exclude dashboard, since the segmentation models don't reliably detect it.)</p>

<p>However, even at this stage,and even with all the very slow plotting driving the FPS down, the results are good enough ...</p>

<p><img src="Screenshot%202023-05-16%20191241.png" style="height: 220px"> <img src="Screenshot%202023-05-16%20191223.png" style="height: 220px"></p>

<p>... that I could imagine doing the following with them:</p>

<ul class="checklist">
<li>
<input type="checkbox" disabled> Set up ROS2 on this machine; maybe in a container.</li>
<li>
<input type="checkbox" disabled> Publish as ROS topics these segmented point clouds in some usable format, with suitably backdated timestamps.</li>
<li>
<input type="checkbox" disabled> Assign the segmented classes to different bins based on how bad (or good) it would be to drive on/through them.</li>
<li>
<input type="checkbox" disabled> Use a Lua mod (see other entries) to</li>
<li>
<input type="checkbox" disabled> Publish acceleration and orientation data as an IMU topic</li>
<li>
<input type="checkbox" disabled> Maybe publish some sparse ray tracings as ultrasound topics.</li>
<li>
<input type="checkbox" disabled> Accept control commands</li>
<li>
<input type="checkbox" disabled> Use e.g. <a href="http://introlab.github.io/rtabmap/">RTAB-Map</a> to integrate these.</li>
</ul>

<p>It might also be worth trying to find a monocular depth network that gives better predictions (<a href="https://huggingface.co/docs/transformers/model_doc/dpt">DPT</a> sounds good?). Alternatively (and this would take a bunch of time), I could use those sparse raytraces to make my own  sparsely-supervised fine-tuning dataset. At the very least, this might help to make the errors less biased (I think the GLPN tends to underestimate).</p>

<p>It could be useful to make use of successive frames, but I can't find any easily available pre-trained nets that do this. <a href="https://github.com/facebookresearch/consistent_depth">This Facebook repo</a> uses single-frame neural depth as the prior for a more conventional SfM method. That might not be too hard to get going, but it seems superficially to be equivalent to just giving the bad point cloud to RTAB-Map with appropriate uncertainties.</p>
</div>
<div class="mdl-cell mdl-cell--2-col mdl-cell--hide-tablet mdl-cell--hide-phone"></div>
</div></main>
</div></body>
</html>
