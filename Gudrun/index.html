<html>
<head>
<link href="https://fonts.googleapis.com/icon?family=Material+Icons" rel="stylesheet">
<link href="https://code.getmdl.io/1.2.1/material.deep_orange-blue.min.css" rel="stylesheet">
<script defer src="https://code.getmdl.io/1.2.1/material.min.js"></script><meta content="width=device-width, initial-scale=1.0" name="viewport">
<style>
.demo-ribbon {
  width: 100%;
  height: 40vh;
  //background-image: url("hero.png");
  background-color: #f5f5f5;
  flex-shrink: 0;
}

.demo-main {
  margin-top: -35vh;
  flex-shrink: 0;
}

.demo-header .mdl-layout__header-row {
  padding-left: 40px;
}

.demo-container {
  max-width: 1600px;
  width: calc(100% - 16px);
  margin: 0 auto;
}

.demo-content {
  border-radius: 2px;
  padding: 80px 56px;
  margin-bottom: 80px;
}

.demo-layout.is-small-screen .demo-content {
  padding: 40px 28px;
}

.demo-content h3 {
  margin-top: 48px;
}

.demo-footer {
  padding-left: 40px;
}

.demo-footer .mdl-mini-footer--link-list a {
  font-size: 13px;
}
#view-source {
    float: right;
}
</style>
<script async="1" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML" type="text/javascript"></script><link href="../styles.css" rel="stylesheet">
<link href="../colorful.css" rel="stylesheet">
<script>
                    function show(toExpand)
                    {
                    if(
                       document.getElementById(toExpand).style.display == 'none'
                       ||
                       document.getElementById(toExpand).style.display == ''
                       )
                        document.getElementById(toExpand).style.display = 'block';
                    else
                        document.getElementById(toExpand).style.display = 'none';
                    }
                    </script><style>
                    div#expand_0{
                        display:none;
                    }

                    div#expand_1{
                        display:none;
                    }

                    div#expand_2{
                        display:none;
                    }

                    div#expand_3{
                        display:none;
                    }

                    div#expand_4{
                        display:none;
                    }

                    div#expand_5{
                        display:none;
                    }</style>
</head>
<body><div class="demo-layout mdl-layout mdl-layout--fixed-header mdl-js-layout mdl-color--grey-100">
<header class="demo-header mdl-layout__header mdl-layout__header--scroll mdl-color--grey-100 mdl-color-text--grey-800"><div class="mdl-layout__header-row">
<span class="mdl-layout-title"><a href="../index.html" style="text-decoration:none; color:#444;" class="mdl-typography--headline">Tom Bertalan</a></span><div class="mdl-layout-spacer"></div>
</div></header><div class="demo-ribbon"></div>
<main class="demo-main mdl-layout__content"><div class="demo-container mdl-grid">
<div class="mdl-cell mdl-cell--2-col mdl-cell--hide-tablet mdl-cell--hide-phone"></div>
<div class="demo-content mdl-color--white mdl-shadow--4dp content mdl-color-text--grey-800 mdl-cell mdl-cell--8-col">
<div class="demo-crumbs mdl-color-text--grey-500">
<a href="../index.html">Home</a> &gt; <span>Gudrun</span><a href="http://github.com/tsbertalan/gudrun" id="view-source" class="mdl-button mdl-js-button mdl-button--raised mdl-js-ripple-effect mdl-color--accent mdl-color-text--accent-contrast">View project files.</a>
</div>
<span></span><h1>Gudrun</h1>

<p><a href="headshot.jpg"><img src="headshot.jpg" width="24%"></a>
<img src="1_small.gif" width="24%">
<img src="2_small.gif" width="24%">
<a href="https://youtu.be/KhBlflgKe1Q?t=81"><img src="teb_demo.gif" width="24%"></a></p>

<p>This robot will be a variant of the <a href="http://www.donkeycar.com/">Donkey car</a>, probably using the <a href="https://hobbyking.com/en_us/trooper-pro-4x4-1-10-brushless-sct-arr.html">recommended chassis</a>. 
However, instead of controlling it with a Raspberry Pi, I'll use <a href="https://pcpartpicker.com/user/tsbertalan/saved/#view=dk9GXL">a computer I built last year</a> (mini-ITX, I think) for a different purpose. I might replace the motherboard with a slim-mini-ITX, a standard which includes a 19VDC power jack. 
Otherwise I'd need something like <a href="https://www.amazon.com/dp/B005TWE6B8/?coliid=I3T66Y7O6B2HJK&amp;colid=3LRY6AZNFBVCM&amp;psc=0&amp;ref_=lv_ov_lig_dp_it">this</a> to provide motherboard power.</p>

<p>Either way, I'll probably need to use a boost converter (which I already have), and I think it should draw about 130W max (according to the estimate from pcpartpicker on the page where you select a power supply). 
<a href="https://hobbyking.com/en_us/multistar-high-capacity-4s-10000mah-multi-rotor-lipo-pack.html">This battery</a> which is currently in Gunnar should be able to handle 100 amps according to <a href="https://www.kritikalmass.net/battery-calculator/index.php">this calculator</a>, so I think this should be fine for about an hour of use--certainly a half-hour. </p>

<p>In a later iteration, I might use the more power-efficient <a href="https://devtalk.nvidia.com/default/topic/1024102/jetson-tx2/jetson-tx2-power-consumption/">Nvidia TX2</a>, but, for this build, I want to minimize the specialness of the computer as much as possible, so that everything is just standard Ubuntu. (The TX2, like the Raspberry Pi, runs a custom linux with an ARM CPU, instead of a "normal" x86_64 CPU, meaning installing software is often harder.)</p>

<p><a href="https://en.wikipedia.org/wiki/Gudrun">Gudrun</a> will be the successor to two previous builds of mine, <a href="https://github.com/tsbertalan/hogni">Hogni</a> and <a href="https://github.com/tsbertalan/gunnar">Gunnar</a>, all three of whom were mythologically siblings. (Also, note to self, if I want to continue this naming scheme, there's a <a href="https://en.wikipedia.org/wiki/Gudrun#Family_relations">good list at the bottom</a> of that article.)</p>
</div>
<div class="mdl-cell mdl-cell--2-col mdl-cell--hide-tablet mdl-cell--hide-phone"></div>
<div class="mdl-cell mdl-cell--2-col mdl-cell--hide-tablet mdl-cell--hide-phone"></div>
<div class="demo-content mdl-color--white mdl-shadow--4dp content mdl-color-text--grey-800 mdl-cell mdl-cell--8-col">
<div class="entry-titles">
<a href="javascript:;" id="view-source" onclick="show('expand_0')" class="mdl-button mdl-js-button mdl-js-ripple-effect">Show/Hide</a><header class="tomsb-entry-header"><h2>Simple ultrasound reactive control</h2>
<p>26 February 2019</p></header>
</div>
<div id="expand_0">
<span></span><h4>Ultrasound for reactive obstacle avoidance</h4>

<p>I added two ultrasound sensors looking off to the left and right 
on the front bumper. An arduino dumps these into a serial terminal at a minimum
of about 5Hz, usually faster), and a ROS node reads these and puts them into two
<a href="http://wiki.ros.org/sensor_msgs"><code>sensor_msgs</code></a><code>.msg.Range</code> topics.</p>

<p>I implemented two behaviors--a back-up-and-turn, 
where we try to go in the direction where there's more space,
and a go-forward, where the steering fraction \(\in(-1,1)\)
is computed like
$$\tanh\left( (d_r - d_l) \cdot \lambda \right)$$
and this noisy output is passed through a 10-entry rolling mean filter
(using <a href="https://docs.python.org/2/library/collections.html#collections.deque"><code>collections.deque</code></a>!).</p>

<p>The result is an illusion of path planning! But it's still really just reactive.</p>

<p><img alt="first video" src="1_small.gif">
<img alt="second video" src="2_small.gif"></p>

<h4>In other news</h4>

<p>I've tested the <a href="http://wiki.ros.org/openni_launch"><code>openni_*</code></a> ROS packages, and found that they produce a surfeit
of depth-camera topics from my first-generation Kinect sensor (the power cable
of which I lopped off and replaced with a barrel connector to my pre-ATX 12V rail).
No accelerometer data, though--it would be nice not to have to add a separate IMU,
and instead just use the one that's in the Kinect. 
I think the <a href="http://wiki.ros.org/kinect_aux"><code>kinect_aux</code></a> package will get this for me.</p>

<p>I hope I can fake "odometry" from this IMU data,
and so obviate the need for separate wheel encoders.
However, if I need them, my current plan is to glue half a dozen tiny magnets
regularly spaced around the inside of the back wheels,
and position a <a href="https://www.sparkfun.com/products/14709">Hall-effect sensor</a>
nearby.
But I'd rather not have to make another mini-project out of getting that little
Arduino-project working properly, reading my poor-man's grey code.
Integrating an IMU in (non-embedded) software would be easier.</p>

<p>As for real planning, I still have some reading to do 
to figure out what's available already-written
for Ackermann-kinematics robots.
I've seen <a href="http://wiki.ros.org/teb_local_planner">TEB local planner</a>
used by others; I'm not sure that this would work with the same global planners
used in the gmapping stack, since some maneuvers, like N-point turns,
are fundamentally different between Ackermann and differential-drive kinematics.
I'm not above writing my own planning software
(actually, writing a quick and dirty pair of MPC-local + tree-based-global
would be a worthwhile endeavor,
and maybe not <em>too</em> much harder than getting existing packages installed,
tuned, and working smoothly),
but first I need at least write a motor controller
that react to <a href="http://wiki.ros.org/ackermann_msgs"><code>ackermann_msgs</code></a><code>.msgs.AckermannDrive</code> messages properly.</p>

<p>And, of course, getting SLAM working with the Kinect 
is a whole separate miniproject.</p>
</div>
</div>
<div class="mdl-cell mdl-cell--2-col mdl-cell--hide-tablet mdl-cell--hide-phone"></div>
<div class="mdl-cell mdl-cell--2-col mdl-cell--hide-tablet mdl-cell--hide-phone"></div>
<div class="demo-content mdl-color--white mdl-shadow--4dp content mdl-color-text--grey-800 mdl-cell mdl-cell--8-col">
<div class="entry-titles">
<a href="javascript:;" id="view-source" onclick="show('expand_1')" class="mdl-button mdl-js-button mdl-js-ripple-effect">Show/Hide</a><header class="tomsb-entry-header"><h2>Architectural plan</h2>
<p>28 February 2019</p></header>
</div>
<div id="expand_1">
<span></span><p>I've done a fair bit of thinking on where I think I should go with this project, and so I'm gathering here my thoughts on what techniques I plan to use in each level of the stack.</p>

<h4>Drivers</h4>

<p>First, I need to be sure that I'm getting all the data I'll need for subsequent steps. This will be the easiest step, and is already mostly done. I want to be able to teleop the car using only these methods, and watching visualizations from these topics, before I proceed.</p>

<ol>
<li>Respond to <a href="http://docs.ros.org/api/ackermann_msgs/html/msg/AckermannDrive.html"><code>AckermannDrive</code> messages</a>, minimally responding to the <code>steering_angle</code> and <code>speed</code> fields. Since the <a href="https://www.pololu.com/product/1351">Pololu Micro Maestro</a> servo controller I'm using to communicate with the car's ESC actually supports ramps with specified speed, a possible stretch goal would be to do something intelligent with the <code>steering_angle_velocity</code>, <code>acceleration</code>, and <code>jerk</code> fields.
   For now, it will be enough to call Pololu's provided <code>UscCmd</code> program with <code>os.system</code>  in our ROS node to set the speed and angle, but their serial protocol is <a href="https://www.pololu.com/docs/0J40/5.c">pretty well documented</a> (summarized for my use <a href="https://github.com/tsbertalan/gudrun/commit/d26840303cfa8fac44f7768aadbfb18fda8f496b">here</a>) so I could avoid the repeated subprocess by sending commands by serial directly.</li>
<li>Use the <a href="http://wiki.ros.org/openni_camera"><code>openni_*</code></a> packages to publish RGBD information. (tested and working)</li>
<li>Use <a href="http://wiki.ros.org/kinect_aux"><code>kinect_aux</code></a> to publish the Kinect's IMU information as a <code>sensor_msgs/Imu</code> topic. (not yet tested)</li>
</ol>

<h4>Perception, sensor fusion, and mapping</h4>

<p>When I have some confidence that I as a human can drive the car using the same sensor data and command topics that I'll be providing to higher-level packages, I can turn to replacing my intuition-based sensor fusion to something more concrete.</p>

<ol>
<li>Use something to fake odometry from the Kinect's IMU data. I think that this may be possible via one of the nodes in <a href="http://docs.ros.org/melodic/api/robot_localization/html/index.html#"><code>robot_localization</code></a>. This is the part I'm most uncertain about. However, since this is superficially a pretty simple task (some interesting notes <a href="http://www.seattlerobotics.org/encoder/200610/Article3/IMU%20Odometry,%20by%20David%20Anderson.htm">here</a>; it might be worthwhile to include our actual motor commands in this reckoning, but for that I'll have to sit down with pencil and paper and work out some (E/U)KF scheme), I might just write my own code to make a best-effort IMU odometer. I've seen gmapping fix some pretty egregious wheel-encoder odometry errors in the past, so I think this will be just <em>fiiiine</em>.</li>
<li>There seem to be many libraries available that will perform SLAM on RGBD or 3D point cloud data. However, after searching for a couple hours, I'm actually having difficulty finding one that will install on ROS Melodic. So, I think I'll take the simpler approach of throwing away most of the RGBD data and instead using one slice as a laserscan via the melodic-available <a href="http://wiki.ros.org/depthimage_to_laserscan"><code>depthimage_to_laserscan</code></a>.
   So, I'll probably  just use this and gmapping. KISS.</li>
</ol>

<h4>Speed control</h4>

<p>I have some doubts here. In <a href="http://tomsb.net/Gunnar/">previous work</a>, I had a tight control loop running on an Arduino to maintain commanded motor speeds. Here, I don't have wheel encoders, so the best I might be able to do is have a loose loop between my (likely very poor) odometry from the perception phase to my motor command interface in the driver phase.</p>

<p>The more I think about this, the more I think that adding some sort of wheel encoders would greatly simplify many other parts of the design. Maybe I should do that Hall-effect sensor side project after all.</p>

<h4>Planning</h4>

<p>Once I can watch the map being generated as I teleop around, I can set up a medium-level planning stack.</p>

<ol>
<li>
<a href="http://wiki.ros.org/teb_local_planner"><code>teb_local_planner</code></a> seems to be the way to go for what they call a local planner. However, the videos there suggest that this planner is capable of fairly advanced maneuvering, with multi-point turns, and it seems that it does consider multiple topologically distinct local plans. </li>
<li>However, TEB does require a <code>nav_msgs/Path</code> global plan, which can be create simply with <a href="http://wiki.ros.org/global_planner?distro=melodic#Published_Topics"><code>global_planner</code></a> (that is, A* or Djikstra, or some clever smoothed, interpolated combination) from the standard <a href="http://wiki.ros.org/navigation">navigation stack</a>.</li>
</ol>

<h4>Behaviors!</h4>

<p>This is the more fun, conceptual part. As suggested by the TEB docs, I'll need to disable some of the low-leve behaviors that come with the navigation stack (like the spin and clear). Witness instead the simple <a href="https://github.com/tsbertalan/gudrun/blob/a8cc65c6957c3498b119dbe4b59c7455c75a2977/src/gudrun_motor/ultrasound_bump_drive.py#L53"><code>Behavior</code> superclass</a> I made, without much thought, for the little bump-drive script I made last week. But, really, this section is more about high-level behaviors. I have a few ideas, some of which require more hardware/software additions than others:</p>

<p><strong>Wander around, and identify and catalog objects.</strong> Fairly easy:</p>

<ol>
<li>
<p>Set random navigation objectives (perhaps with some <a href="http://wiki.ros.org/frontier_exploration">frontier exploration</a> strategy, though that's pretty optional).</p>
</li>
<li>
<p>Wander around, take photographs annotated with current poses (and therefore, ideally, the photographed objects position--the RGBD might help a lot with this).</p>
</li>
<li>
<p>Pass them to some pre-trained object recognition neural network (I have a <a href="https://coral.withgoogle.com/">Google Edge TPU</a> I want to try to use for this), and note any high-confidence hits.</p>
</li>
<li>
<p>Assemble a database from these and make a nice frontend for querying it. Maybe a visual menu, and and for each item, there'd be a "take me there" button. The robot would drive to the remembered pose (including orientation), and then (stretch goal!), use a pair of servos to direct a laser pointer at the remembered position of the object (or even redetect it live and point to the object's updated location).</p>
</li>
</ol>

<p><strong>Patrol.</strong> In addition to or in place of the previous goal, we'd explore as far as possible (e.g., a whole apartment), and then continuously revisit the areas we saw the least recently. A nice trick would be to do some sort of anomaly detection. Somehow featureize all the views of the apartment (probably just camera+pose) and then continuously do some unsupervised learning to detect when these features go away from the typical. Obviously, the border of the "typical" region in feature space will become better-characterized as we gather more data, and there will be lots of false positives at first (it's dark! this is unfamiliar! I'm afraid!).</p>

<p><strong>Recharge.</strong> In addition to either or both of the previous, it would be great if Gudrun could recharge herself. I have had some thoughts on this, but put it on hold as overcomplicated and not necessary for now. Basically, though, my thoughts are divided between</p>

<ol>
<li>
<p>loading onto the underdeck a pair of simple NiMH and Li-Ion (balancing) battery chargers designed for standalone use, and then using a complex arrangement of transistors, relays, and ADC voltage sensing to flip power over suddenly from battery to an external bumper; and</p>
</li>
<li>
<p>wrapping the batteries in some <a href="https://hobbyking.com/en_us/6s-li-ion-10a-pcm.html">battery management system</a>, (preferably with balancing capability), and trying to avoid letting the batteries ever get so low that they need a proper "charger". Basically, float-charging them at below nominal voltage. I have a couple of ~5V solar panels that might contribute here, especially if I do ...</p>
</li>
</ol>

<p><strong>Outdoor path following.</strong> Here, the more focused goals of above might be discarded in place of just being able to navigate reliably across campus. However, an issue with this is that the view distances are typically <em>much</em> longer than in indoor scenes, and so path planning is very different. However, in a way, there's more opportunity for creativity here, since it's less geometric and special-senor based, and more computer vision. Really, though, since this would involve significant changes to the above stack, it would almost be separate project. But, as the weather gets warmer, this might beckon more strongly...</p>
</div>
</div>
<div class="mdl-cell mdl-cell--2-col mdl-cell--hide-tablet mdl-cell--hide-phone"></div>
<div class="mdl-cell mdl-cell--2-col mdl-cell--hide-tablet mdl-cell--hide-phone"></div>
<div class="demo-content mdl-color--white mdl-shadow--4dp content mdl-color-text--grey-800 mdl-cell mdl-cell--8-col">
<div class="entry-titles">
<a href="javascript:;" id="view-source" onclick="show('expand_2')" class="mdl-button mdl-js-button mdl-js-ripple-effect">Show/Hide</a><header class="tomsb-entry-header"><h2>Open-loop control of speed</h2>
<p>2 March 2019</p></header>
</div>
<div id="expand_2">
<span></span><h3>Predicting speed from throttle</h3>

<p>I expect that, eventually, I will need closed-loop control of speed. So, I've ordered some <a href="https://www.amazon.com/gp/product/B07BHF3X86/ref=ppx_yo_dt_b_asin_title_o00_s00?ie=UTF8&amp;psc=1">tiny 2 mm cylindrical magnets</a> and <a href="https://www.sparkfun.com/products/14709">Hall-effect sensors</a>, and later I'll try gluing the magnets onto the inside of my rear wheels, watching for their passage with the hall effect sensors, and getting a crude measurement of velocity from that.</p>

<p>But, for now, I want to know simply what are <em>reasonable</em> throttle values to use for a given requested velocity. This is a crude form of open-loop control: give the control action that your internal model says <em>should</em> be right under the current circumstances to achieve the desired process output.</p>

<p>At first, I dutifully made a launchfile that drove forward at a specified throttle for a couple seconds, and started measuring driven distances with my tape measure. However, I quickly realized that I could get a lot more data at higher quality if I used my ultrasound, so I wrote a <a href="https://github.com/tsbertalan/gudrun/blob/82daeba2927c37f7dfff13ec2c8df2948180a413/src/gudrun_motor/launch/speed_record.launch">different launch file</a> that also started the ultrasound, and recorded time, throttle, steering, and ultrasound measurements to a <a href="https://github.com/tsbertalan/gudrun/blob/82daeba2927c37f7dfff13ec2c8df2948180a413/src/gudrun_motor/speed_record_data/speed_record.csv">CSV file</a>.</p>

<p>I then loaded this data into a <a href="https://github.com/tsbertalan/gudrun/blob/82daeba2927c37f7dfff13ec2c8df2948180a413/src/gudrun_motor/speed_record_data/Speed%20Record%20Data.ipynb">Jupyter notebook</a>. It quickly became clear that I could split up my experiments by looking for big spikes in the \(\Delta t\) between ultrasound measurements.</p>

<p>And then, for each experiment, there were some clear outliers--caused, I imagine, by the ultrasound picking up some spurious echoes. I rejected these in my regression by using <a href="https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.RANSACRegressor.html">Scikit-learn's RANSAC linear regressor</a>, and tuning the <code>residual_threshold</code> parameter to reject what, to my eye, seemed appropriate for rejection in some spot checks.</p>

<p><img src="sample_experiment.png" width="100%/"></p>

<p>In the experiment shown above, you can see that our distance to the wall decreases steadily as our fixed throttle maintains fixed speed (in equilibrium against the rolling resistance, on flat ground), until we eventually hit the wall (or, more accurately, we first hit the <a href="https://github.com/tsbertalan/gudrun/blob/82daeba2927c37f7dfff13ec2c8df2948180a413/src/gudrun_sensors/listen_to_ultrasound.py#L34"><code>min_range</code></a> for our ultrasound topic), leading to some outliers at the right side of the plot (around \(t=1086\) seconds)</p>

<p>The important thing to get from this experiment at a throttle value of 0.24 was the slope of this line--the centimeters per second value. I managed to get 23 other such pairs from the data, and created a calibration curve.</p>

<p><img src="throttle_rates.png" width="100%/"></p>

<p>Here, I again did two more regressions, and, again, I used RANSAC to throw away one obvious outliers. I probably could have just done this manually, but I might want to get more data in the future&#8212;especially in the forward direction--and this approach generalizes better. One important thing to notice here is that there is a significant dead zone in the throttles. I could perhaps characterize this better by gathering some more data in the approach to and beyond the transition into the dead zone, especially in the forward direction.</p>

<p>Another thing that might be worth doing is dumping the ultrasound for this and instead using my fancy depth camera--this will be less susceptible to the echo problem of the ultrasound (which got worse as I got further from the wall). However, with the RANSAC to reject outliers, it may be possible to get significantly further from the wall than the 1.2 meters or so that I was doing in the above batch of experiments. This is important because it would enable taking data at higher speeds. </p>

<p>(Here the point where I admit that, at one point, I disabled my <a href="https://github.com/tsbertalan/gudrun/blob/82daeba2927c37f7dfff13ec2c8df2948180a413/src/gudrun_motor/speed_record.py#L27"><code>MAX_THROTTLE_ABS</code></a> safeguard, and then, due to a bad command line, ended up running into the wall at max throttle, with about 15 feet of runway to get up to speed. Luckily, I had a spare <a href="https://www.sparkfun.com/products/13959">ultrasound sensor</a> to replace the one that seemed to stop working at this point, and I was back online in minutes.)</p>

<h3>Predicting turning radius from steering effort</h3>

<p>In addition to the throttle calibration curve, I want to know what steering effort \( w \in [-1,1] \) to apply to obtain a particular turning rate, as measured by our instantaneous turning radius.</p>

<p>For this, I did a simpler experiment: I simply ran the car forward for four seconds at a fixed throttle for each of nine steering effort values, resetting the position before each. I marked the beginning and ending positions of the tire fronts, making for eighteen marked points (0 through 17), which can be divided into nine pairs (A through I). I used <a href="https://apps.automeris.io/wpd/">an online tool</a> to extract coordinates from this image. </p>

<p>For each pair, I compute the center (mean) point. Then, given a driving arclength \(d\), and a turning radius \(r=f/w\), where \(f\) is the scaling factor to be found. I compute predicted center points as</p>

<p>$$x=r + r \cos(\pi - \theta)$$</p>

<p>$$y=r \sin(\pi - \theta)$$</p>

<p>for turns to the right, and</p>

<p>$$x=-r+r \cos(\theta)$$</p>

<p>$$y=-r+r\sin(\theta)$$</p>

<p>for turns to the left (where \(\theta=d/2/r\) is the faction of the full \(2\pi\) of the circumference that we drive in our \(d\)).</p>

<p>I then find the mean distance from these predictions to the actual endpoints, and minimize this loss function by tuning \(d\) and \(w\) using <code>spicy.optimize.minimize</code>.</p>

<p><img alt="steering results" src="steering.gif"></p>

<p>There are some definite problems with this approach. In truth, there are five circles of different radii involved at any instant when the wheel of an Ackermann car is not centered: different circles for each of the four wheels, and one virtual circle that begins at the center of the back axel, which is the radius we actually want to talk about. So, a better way to do all of this would have been to mark the locations of the <em>back</em> wheels after each maneuver, or at least approximate these positions from our marked values. But this will be good enough for now.</p>

<p>However, tonight, my Hall effect sensors arrived, so I can next get some <em>real</em> measurments of distance traveled.</p>
</div>
</div>
<div class="mdl-cell mdl-cell--2-col mdl-cell--hide-tablet mdl-cell--hide-phone"></div>
<div class="mdl-cell mdl-cell--2-col mdl-cell--hide-tablet mdl-cell--hide-phone"></div>
<div class="demo-content mdl-color--white mdl-shadow--4dp content mdl-color-text--grey-800 mdl-cell mdl-cell--8-col">
<div class="entry-titles">
<a href="javascript:;" id="view-source" onclick="show('expand_3')" class="mdl-button mdl-js-button mdl-js-ripple-effect">Show/Hide</a><header class="tomsb-entry-header"><h2>Timed-elastic-band path planning</h2>
<p>3 April 2019</p></header>
</div>
<div id="expand_3">
<span></span><p>I managed to get TEB local planner working on Gudrun, though, as expected, the visual-only odometry makes the whole stack brittle. But what really matters is that we now have some sort of autonomous waypoint-following. <a href="https://youtu.be/KhBlflgKe1Q">Full video here</a>.</p>

<p>Code as of this writing is at <a href="https://github.com/tsbertalan/gudrun/commit/caddca1ca670a086d93dbaf3a1567bc4891dde9b">commit caddca1</a>.</p>

<p><img alt="teb_demo" src="teb_demo.gif"></p>

<iframe width="560" height="315" src="https://www.youtube.com/embed/KhBlflgKe1Q" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>

<p>This uses a fairly simple TF tree, half of which is internal transforms used by the Kinect.</p>

<p><a href="frames.png"><img src="frames.png" width="100%"></a></p>

<p>The rqt_graph is a little more complicated, but only superficially so.</p>

<p><a href="rosgraph.svg"><img src="rosgraph.svg" width="100%"></a></p>

<p>We have a chain of input from left to right--sensing (<code>/camera</code> and <code>/listen_to_encoder</code>), perception (<code>/rtabmap</code>), planning (<code>/move_base</code>, in particular <code>/move_base/TebLocalPlannerROS</code>), and finally actuation (<code>/ackermann_motor_controller</code>).</p>

<p>The planning process goes roughly like this: </p>

<p>RTAB-map collects the RGBD data from the Kinect 1 sensor (generated using OpenNI), and performs SLAM with this data. Odometry at the moment is purely visual--no information is incorporated from the Kinect's onboard accelerometer, or from the wheel encoders. RTAB-map classifies the points it finds into obstacles and ground (based, I believe, on the angle of the local tangent vector from vertical). The obstacle points are projected down into a 2D occupancy grid.</p>

<p><code>/move_base</code> subscribes to this grid topic, and then uses a heuristic planner (think A*) to generate a rough "global" path through this grid to the goal.</p>

<p>The TEB ("timed elastic band") looks a short distance along this global plan, and then creates a more kinematically realistic local plan that approximates this snippet of the global plan. This is done through a few iterations of an optimizer, which considers many objectives together. The resulting local plans obey, among other things, the minimum turning radius parameter for the robot, which is crucial for Ackermann steering. So, these plans often include multi-point turn maneuvers.</p>

<p>One of the objectives the TEB planner seeks to satisfy is a lack of collisions! The naive way to compute these is to check whether our bounding box would include any of points in the occupancy grid. Instead, the points nearest to the trajectory are coalesced into local clusters, and these are fit with polygonal shapes using a RANSAC (outlier-rejecting) regressor. This results in a much smaller, presumably geometrically analytical, set of conditions to check for collisions.</p>

<p>Finally, we take the <code>/cmd_vel</code> topic emitted by <code>/move_base</code> (this should be just the first control action along the TEB local path calculated at this moment), transform it into equivalent Ackermann steering angle and rear axle velocity, and pass this on to our speed/steering controller. This uses the handy <a href="https://pypi.org/project/simple-pid/">simple-pid Python package</a> to keep to an assigned speed set point, though we restrict the controller output to be positive (and only give it absolute speed feedback), and instead switch the sign of the throttle according to the requested drive direction. If I had access to both channels from the encoder disk magnet, I could additionally report instantaneous direction of movement as well as magnitude, and the controller could handle the whole thing. But, anyway, this basically how it works in a real car (you can't just go instantly from forward to reverse, though braking (negative "throttle" with positive velocity) is a thing).</p>

<p>I think I'll need to improve the odometry before I build too much more on this somewhat shaky base, but before I do that, I'll have to try it outside at least once!</p>
</div>
</div>
<div class="mdl-cell mdl-cell--2-col mdl-cell--hide-tablet mdl-cell--hide-phone"></div>
<div class="mdl-cell mdl-cell--2-col mdl-cell--hide-tablet mdl-cell--hide-phone"></div>
<div class="demo-content mdl-color--white mdl-shadow--4dp content mdl-color-text--grey-800 mdl-cell mdl-cell--8-col">
<div class="entry-titles">
<a href="javascript:;" id="view-source" onclick="show('expand_4')" class="mdl-button mdl-js-button mdl-js-ripple-effect">Show/Hide</a><header class="tomsb-entry-header"><h2>Reading encoder speed</h2>
<p>8 April 2019</p></header>
</div>
<div id="expand_4">
<span></span><p>After my old encoder (salvaged from a stripped-out <a href="https://www.pololu.com/product/3207">Pololu gearmotor</a>) flew apart under centrifugal strain when I accidentally set the motors to full-speed with my bad ESC code, I made a replacement using <a href="https://www.amazon.com/dp/B07K8WHKWX">this kit, also from Pololu</a>.</p>

<p>Since the shaft of the motor was a little too big for the toroidal magnet, I <em>gently</em> drilled it out a bit. But then the hole was a little too large. Hoping for the best (so far, my luck seems to be holding), I simply super-glued it to the shaft.</p>

<p>The next problem was mounting the Hall-effect sensor carrier board near enough to magnet to pick it up. I had already discarded the idea of mounting it under the magnet, as designed (I guess I might have just gotten lucky with the mounting orientation), since I wanted to keep this thing as disassembleable as possible. I also didn't want to create a frame and glue the board in place like I did the first time, but have to carefully adjust the gap while the glue dried.</p>

<p>Instead, I settled on a design I'm somewhat proud of, that has two adjustable spring-driven height screws. The white plastic pieces, like the struts I cut to replace the car's shocks, are from the housing of a discarded miniblind that I salvaged. A dremel-like cutter, a cordless drill, some glue, and some assorted small pieces of mounting hardware like this are so far an able replacement for a 3D printer for me. In the first design, I warmed the miniblind-plastic with a lighter and then bent it into the U-shape for the bracket, but this tended to weaken it at the corners (and was also a permanent attachment to the motor). Here, I can remove the two side bolts and detach all of the assembly except for the magnet and the two glued-on white side posts.</p>

<p><img width="100%" src="encoder_adjustable.jpg"></p>

<p>I verified that the sensor board was in fact producing a reasonable square wave as the motor spun, and on both channels.</p>

<p><img width="100%" src="oscilloscope-crop.jpg"></p>

<p>I then wrote some <a href="https://github.com/tsbertalan/gudrun/blob/b4e12fda30f60cfaba28d79f4093eb3874b4b65c/src/gudrun_motor/encoder/encoder.ino">fairly simple firmware</a> to watch for these rising and falling edges with interrupts, and then either increment or decrement a counter depending on the state of the two square waves, and their previous state. I could have done this with a bunch of if statements, and it would likely be just as efficient as compilation, but I liked the elegance of doing a little simple bitwise math here, with <code>symbol = b + (a &lt;&lt; 1)</code>.</p>

<div class="codehilite"><pre><span></span><span class="k">volatile</span> <span class="kt">long</span> <span class="n">count</span><span class="p">;</span>
<span class="k">volatile</span> <span class="n">byte</span> <span class="n">last_symbol</span><span class="p">,</span> <span class="n">symbol</span><span class="p">,</span> <span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">;</span>

<span class="k">const</span> <span class="kt">int</span> <span class="n">PIN_A</span> <span class="o">=</span> <span class="mi">2</span><span class="p">;</span>
<span class="k">const</span> <span class="kt">int</span> <span class="n">PIN_B</span> <span class="o">=</span> <span class="mi">3</span><span class="p">;</span>

<span class="c1">// Current byte     -&gt;       0b00, 0b01, 0b10, 0b11</span>
<span class="c1">// Previous byte:</span>
<span class="k">const</span> <span class="n">byte</span> <span class="n">fwd_sources</span><span class="p">[</span><span class="mi">4</span><span class="p">]</span> <span class="o">=</span> <span class="p">{</span><span class="mi">0</span><span class="n">b10</span><span class="p">,</span> <span class="mi">0</span><span class="n">b00</span><span class="p">,</span> <span class="mi">0</span><span class="n">b11</span><span class="p">,</span> <span class="mi">0</span><span class="n">b01</span><span class="p">};</span>
<span class="k">const</span> <span class="n">byte</span> <span class="n">rev_sources</span><span class="p">[</span><span class="mi">4</span><span class="p">]</span> <span class="o">=</span> <span class="p">{</span><span class="mi">0</span><span class="n">b01</span><span class="p">,</span> <span class="mi">0</span><span class="n">b11</span><span class="p">,</span> <span class="mi">0</span><span class="n">b00</span><span class="p">,</span> <span class="mi">0</span><span class="n">b10</span><span class="p">};</span>

<span class="kt">void</span> <span class="nf">isr</span><span class="p">()</span> <span class="p">{</span>
    <span class="n">a</span> <span class="o">=</span> <span class="n">digitalRead</span><span class="p">(</span><span class="n">PIN_A</span><span class="p">);</span>
    <span class="n">b</span> <span class="o">=</span> <span class="n">digitalRead</span><span class="p">(</span><span class="n">PIN_B</span><span class="p">);</span>
    <span class="n">symbol</span> <span class="o">=</span> <span class="n">b</span> <span class="o">+</span> <span class="p">(</span><span class="n">a</span> <span class="o">&lt;&lt;</span> <span class="mi">1</span><span class="p">);</span>
    <span class="k">if</span><span class="p">(</span><span class="n">fwd_sources</span><span class="p">[</span><span class="n">symbol</span><span class="p">]</span> <span class="o">==</span> <span class="n">last_symbol</span><span class="p">)</span> <span class="p">{</span>
        <span class="n">count</span><span class="o">++</span><span class="p">;</span>
    <span class="p">}</span>
    <span class="k">if</span><span class="p">(</span><span class="n">rev_sources</span><span class="p">[</span><span class="n">symbol</span><span class="p">]</span> <span class="o">==</span> <span class="n">last_symbol</span><span class="p">)</span> <span class="p">{</span>
        <span class="n">count</span><span class="o">--</span><span class="p">;</span>
    <span class="p">}</span>
    <span class="n">last_symbol</span> <span class="o">=</span> <span class="n">symbol</span><span class="p">;</span>   
<span class="p">}</span>

<span class="kt">void</span> <span class="nf">setup</span><span class="p">()</span> <span class="p">{</span>
    <span class="n">Serial</span><span class="p">.</span><span class="n">begin</span><span class="p">(</span><span class="mi">115200</span><span class="p">);</span>
    <span class="n">pinMode</span><span class="p">(</span><span class="n">PIN_A</span><span class="p">,</span> <span class="n">INPUT_PULLUP</span><span class="p">);</span>
    <span class="n">pinMode</span><span class="p">(</span><span class="n">PIN_B</span><span class="p">,</span> <span class="n">INPUT_PULLUP</span><span class="p">);</span>
    <span class="n">attachInterrupt</span><span class="p">(</span><span class="n">digitalPinToInterrupt</span><span class="p">(</span><span class="n">PIN_A</span><span class="p">),</span> <span class="n">isr</span><span class="p">,</span> <span class="n">CHANGE</span><span class="p">);</span>
    <span class="n">attachInterrupt</span><span class="p">(</span><span class="n">digitalPinToInterrupt</span><span class="p">(</span><span class="n">PIN_B</span><span class="p">),</span> <span class="n">isr</span><span class="p">,</span> <span class="n">CHANGE</span><span class="p">);</span>
    <span class="n">count</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span>
<span class="p">}</span>
<span class="kt">void</span> <span class="nf">loop</span><span class="p">()</span> <span class="p">{</span>
    <span class="k">if</span> <span class="p">(</span> <span class="n">millis</span><span class="p">()</span> <span class="o">%</span> <span class="mi">10</span> <span class="o">==</span> <span class="mi">0</span><span class="p">)</span> <span class="p">{</span>
        <span class="n">Serial</span><span class="p">.</span><span class="n">println</span><span class="p">(</span><span class="n">count</span><span class="p">);</span>
    <span class="p">}</span>
<span class="p">}</span>
</pre></div>

<p>That is, there are two distinct sequences of two-bit symbols for foward or reverse motion, and we can read these symbols into a single byte (a smaller type would also do, but apparently <a href="https://en.wikipedia.org/wiki/Units_of_information#Obsolete_and_unusual_units">semi-nibbles</a> aren't a standard compiler-recognized type). Since we can also index with these, I make two tiny look-up-tables for what byte we would be coming from, given the current byte as index, in the two hypothetical situations where we're going forward or reverse. If neither matches (probably damaged hardware, or a bad gap size between magnet and sensor, interference by the <code>Serial</code> call, or something like that), we do nothing.</p>

<p>I then dumped this count periodically to serial. For this, I just used <code>Serial.prinln</code>. For the various Arduino data interfaces in this project, I seem to use a different ad-hoc protocol each time, sometimes with good efficiency (sending structs as byte sequences), sometimes reliability (requiring valid checksum after the . data). This way has neither of these properties. Eventually, I should unify all these in a tiny protocol, which could probably sit in a single short header file, and accompanying Python module. But whatever, this is good enough for now.</p>

<p>In the <a href="https://github.com/tsbertalan/gudrun/blob/b4e12fda30f60cfaba28d79f4093eb3874b4b65c/src/gudrun_motor/listen_to_encoder.py">receiving Python code</a>, I continuously append read <code>count</code> values and corresponding times-of-reception into two <code>collections.deque</code> objects, to keep a rotating buffer, and then use <code>counts_per_second, unused_intercept = np.polyfit(self.last_times, self.last_counts, 1)</code> to fit a line to these, whose slope is the desired speed. The size of the two <code>deque</code>s therefore implicitly smooths the incoming data. I then do some unit conversions to get meters-per-second, and send this off to a <code>Float32</code> ROS topic.</p>
</div>
</div>
<div class="mdl-cell mdl-cell--2-col mdl-cell--hide-tablet mdl-cell--hide-phone"></div>
<div class="mdl-cell mdl-cell--2-col mdl-cell--hide-tablet mdl-cell--hide-phone"></div>
<div class="demo-content mdl-color--white mdl-shadow--4dp content mdl-color-text--grey-800 mdl-cell mdl-cell--8-col">
<div class="entry-titles">
<a href="javascript:;" id="view-source" onclick="show('expand_5')" class="mdl-button mdl-js-button mdl-js-ripple-effect">Show/Hide</a><header class="tomsb-entry-header"><h2>Fusing Adafruit 9DoF data with imu_filter_madgwick</h2>
<p>14 April 2019</p></header>
</div>
<div id="expand_5">
<span></span><p><img src="imu_rot.gif" width="100%"></p>

<p>In order to improve upon the visual-only odometry from RTAB-Map, I want to use <code>robot_localization</code> to fuse in accelerometer, gyroscope, wheel encoder/steering angle, and maybe magnetometer information. Step one in this is getting reliable accelerometer/gyroscope data from an IMU. I've had a <a href="https://www.adafruit.com/product/1714">Adafruit 9DoF IMU</a> on hand for a few years, without really making use of it. So, I soldered it to an Arduino Pro Micro, and bolted the breadboard holding both to the car's frame, in an orientation that was easy to express with a manually-constructed <code>static_transform_publisher</code> (visible in the gif above) (due to my sloppy construction, this might not be exactly ground-truth accurate, but it's Good Enough).</p>

<p><a href="imu_package.jpg"><img src="imu_package.jpg" width="100%/"></a></p>

<p>As an aide, these Pro Micros are quickly becoming a favorite of mine for making little USB peripherals. You can get them for about $8 each in packs of three; unlike Pro Minis, they have onboard USB; and they're faster than Nanos (Micro:Nano::Leonardo::Uno). Using a <a href="https://github.com/tsbertalan/gudrun/blob/master/src/gudrun_sensors/upload_with_specified_vid_pid.py">fairly ugly hack</a> of modifying the boards.txt file before flashing, and restoring the original afterwards, I can modify the vendor and product ID of the devices, and then <a href="https://github.com/tsbertalan/gudrun/blob/master/src/gudrun_sensors/get_usb_device_by_ID.py">search for the corresponding USB device later</a> when I want to connect it. So, I have now three of these little self-contained USB devices (IMU, encoders, and ESC interface) connected to the main board by six-inch cables (both ends for the IMU are visible in the above photo), and all can be unambiguously identified on every boot, though the <code>/dev/ttyAM*</code> order might change.</p>

<p>I then started experimenting with some firmware (sorry, "sketches"). After experimenting a bit, I became dissatisfied with the <a href="https://github.com/adafruit/Adafruit_9DOF/blob/master/Adafruit_9DOF.cpp#L317">provided (stateless) sensor fusion code</a>. I may have just been using it wrong, but this code fuses the orientation into "Euler angles", without specifying what convention this used, or in what frame these angles were written, and didn't make any use of the on-board magnetometer.</p>

<p>However, what is clear according to <a href="http://www.ros.org/reps/rep-0145.html">REP-145</a> is the "sensor frame" for the linear acceleration and angular velocity. Linear acceleration is a translation-like quantity, and so simple to transform. Angular velocity, despite the name, is <em>not</em>  a rotation-like quantity. "The rotational velocity is right handed with respect to the body axes, and independent of the orientation of the device." In fact, this is just what is already reported by the <a href="https://github.com/adafruit/Adafruit_LSM303DLHC/blob/master/Adafruit_LSM303_U.cpp#L180">accelerometer's</a> and <a href="https://github.com/adafruit/Adafruit_L3GD20_U/blob/master/Adafruit_L3GD20_U.cpp#L253">gyroscope's</a> respective <code>getEvent</code> methods.</p>

<p>For that matter, this is also true of the <a href="https://github.com/adafruit/Adafruit_LSM303DLHC/blob/master/Adafruit_LSM303_U.cpp#L435">magnetometer</a>, though I've decided not to use it for the time being. To use it properly, I'd need to do some calibration to remove the influence of the motor's magnetism, which I don't want to do right now. (See below.)</p>

<p>So, I wrote firmware that packaged up the accelerometer, gyroscope, and magnetometer into a 9-float struct, preceded that by an <code>S</code> byte, used <code>serial.write</code> to put that onto the USB, and then used an <code>E</code> byte to mark the end of the packet. I read this into Python on the PC side using <code>floats = struct.unpack('&lt;fffffffff', bytes)</code>, did some unit conversions, and populated my ROS messages with this data.</p>

<p>To keep my options open, I also wrote code to publish the firmware-fused orientation, using <code>tf.transformations.quaternion_from_euler</code> to convert it to the quaternion expected for <code>Imu.orientation.[x,y,z,w]</code>. However, <code>quaternion_from_euler</code> rightly has a second argument for specifiying which of the 24 valid Euler conventions the input uses, and I don't know what to tell it. So, I suspect the generated quaternion might be garbage, but haven't checked. What I <em>have</em>  checked is the publish rate with and without this option enabled, and it seems that this brings us from 260 Hz down to about 150 Hz. So, It's disable for now.</p>

<p>I monitored the data from this pipeline-so-far in rqt, and found that, at rest, the three gyro components were nonzero. So, I subtracted their unadjusted quiescent values from the device-reported values before publishing. Without this, there was steady and visible heading-drift over time.</p>

<p>Amazingly, after tuning the gyro offsets, I tried simply breathing on the chip, and this was enough to cause a sudden drift.</p>

<p><img width="100%" src="breathed_on_gyro.png"></p>

<p>Note of course that the vertical scale here is still quite small. (The horizontal axis in all these rqt plots is time in seconds.) It takes about four minutes to come back to its previous temperature, judging by the time it takes for the slope to return to its previous value. Interestingly, when I made the above plot, the slope before breathing was, due to my calibrations, nearly level on this 0.15 vertical scale. As I write this later in the evening, it's neutral slope is decreasing. At first, I thought this was due to the cooling of the evening, and that of course a temperature calibration curve should be written for this (which is what the the extra DoF in the Adafruit 10 DoF would be for). </p>

<p><img width="100%" src="breathed%203.png"></p>

<p>However, after I let it run over night, it seemed more that the varying slopes (in the orientation channel, at least) were just due to the regular, very slow rotation (with a period of about 1.4 hours).</p>

<p><img width="100%" src="long%20drift.png"></p>

<p>A better test then would be to record not the fused heading <code>/imu/data/orientation/z</code>, but the raw gyroscope data. Maybe later I'll try recording that over a 24 hour period. Plot, say, the absolute value of the gyro readings on a log scale.</p>

<p><img width="100%" src="breathed_2.png"></p>

<p>This quiescent drift would certainly go away if we incorporated a magnetometer.</p>

<p><img width="100%" src="return%20after%20breathing%20with%20mag.png"></p>

<p>Here, I breathed on the sensor at about t=40 seconds, and it returned to its previous orientation (and I expect it will hold this position as long as the magnetic field nearby holds steady).</p>

<p><img width="100%" src="gyro%20no%20drift%20with%20mag.png"></p>

<p>But that ostensibly "global" orientation is likely not one in which North is correct, since a big portion of the sensed magnetic field is due to the nearby permanent magnet in the car's motor. So, I'll need to calibrate that away before setting <code>use_mag=True</code> in the <code>imu_filter_madgwick</code> settings.</p>

<p>Now, I need to transform the wheel speed/turning angle into something like a <code>Twist</code> odometry message, and get all this data into <code>robot_localization</code>.</p>
</div>
</div>
<div class="mdl-cell mdl-cell--2-col mdl-cell--hide-tablet mdl-cell--hide-phone"></div>
</div></main>
</div></body>
</html>
