<html>
<head>
<link href="https://fonts.googleapis.com/icon?family=Material+Icons" rel="stylesheet">
<link href="https://code.getmdl.io/1.2.1/material.deep_orange-blue.min.css" rel="stylesheet">
<script defer src="https://code.getmdl.io/1.2.1/material.min.js"></script><meta content="width=device-width, initial-scale=1.0" name="viewport">
<style>
.demo-ribbon {
  width: 100%;
  height: 40vh;
  //background-image: url("hero.png");
  background-color: #f5f5f5;
  flex-shrink: 0;
}

.demo-main {
  margin-top: -35vh;
  flex-shrink: 0;
}

.demo-header .mdl-layout__header-row {
  padding-left: 40px;
}

.demo-container {
  max-width: 1600px;
  width: calc(100% - 16px);
  margin: 0 auto;
}

.demo-content {
  border-radius: 2px;
  padding: 80px 56px;
  margin-bottom: 80px;
}

.demo-layout.is-small-screen .demo-content {
  padding: 40px 28px;
}

.demo-content h3 {
  margin-top: 48px;
}

.demo-footer {
  padding-left: 40px;
}

.demo-footer .mdl-mini-footer--link-list a {
  font-size: 13px;
}
#view-source {
    float: right;
}
</style>
<script async="1" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML" type="text/javascript"></script><script>
                    function show(toExpand)
                    {
                    if(
                       document.getElementById(toExpand).style.display == 'none'
                       ||
                       document.getElementById(toExpand).style.display == ''
                       )
                        document.getElementById(toExpand).style.display = 'block';
                    else
                        document.getElementById(toExpand).style.display = 'none';
                    }
                    </script><style>
                    div#expand_0{
                        display:none;
                    }

                    div#expand_1{
                        display:none;
                    }

                    div#expand_2{
                        display:none;
                    }</style>
</head>
<body><div class="demo-layout mdl-layout mdl-layout--fixed-header mdl-js-layout mdl-color--grey-100">
<header class="demo-header mdl-layout__header mdl-layout__header--scroll mdl-color--grey-100 mdl-color-text--grey-800"><div class="mdl-layout__header-row">
<span class="mdl-layout-title"><a href="../index.html" style="text-decoration:none; color:#444;" class="mdl-typography--headline">Tom Bertalan</a></span><div class="mdl-layout-spacer"></div>
</div></header><div class="demo-ribbon"></div>
<main class="demo-main mdl-layout__content"><div class="demo-container mdl-grid">
<div class="mdl-cell mdl-cell--2-col mdl-cell--hide-tablet mdl-cell--hide-phone"></div>
<div class="demo-content mdl-color--white mdl-shadow--4dp content mdl-color-text--grey-800 mdl-cell mdl-cell--8-col">
<div class="demo-crumbs mdl-color-text--grey-500">
<a href="../index.html">Home</a> &gt; <span>Gudrun</span><a href="http://github.com/tsbertalan/gudrun" id="view-source" class="mdl-button mdl-js-button mdl-button--raised mdl-js-ripple-effect mdl-color--accent mdl-color-text--accent-contrast">View project files.</a>
</div>
<span></span><h1>Gudrun</h1>
<p><a href="headshot.jpg"><img src="headshot.jpg" width="30%"></a>
<img src="1_small.gif" width="30%">
<img src="2_small.gif" width="30%"></p>
<p>This robot will be a variant of the <a href="http://www.donkeycar.com/">Donkey car</a>, probably using the <a href="https://hobbyking.com/en_us/trooper-pro-4x4-1-10-brushless-sct-arr.html">recommended chassis</a>. 
However, instead of controlling it with a Raspberry Pi, I'll use <a href="https://pcpartpicker.com/user/tsbertalan/saved/#view=dk9GXL">a computer I built last year</a> (mini-ITX, I think) for a different purpose. I might replace the motherboard with a slim-mini-ITX, a standard which includes a 19VDC power jack. 
Otherwise I'd need something like <a href="https://www.amazon.com/dp/B005TWE6B8/?coliid=I3T66Y7O6B2HJK&amp;colid=3LRY6AZNFBVCM&amp;psc=0&amp;ref_=lv_ov_lig_dp_it">this</a> to provide motherboard power.</p>
<p>Either way, I'll probably need to use a boost converter (which I already have), and I think it should draw about 130W max (according to the estimate from pcpartpicker on the page where you select a power supply). 
<a href="https://hobbyking.com/en_us/multistar-high-capacity-4s-10000mah-multi-rotor-lipo-pack.html">This battery</a> which is currently in Gunnar should be able to handle 100 amps according to <a href="https://www.kritikalmass.net/battery-calculator/index.php">this calculator</a>, so I think this should be fine for about an hour of use--certainly a half-hour. </p>
<p>In a later iteration, I might use the more power-efficient <a href="https://devtalk.nvidia.com/default/topic/1024102/jetson-tx2/jetson-tx2-power-consumption/">Nvidia TX2</a>, but, for this build, I want to minimize the specialness of the computer as much as possible, so that everything is just standard Ubuntu. (The TX2, like the Raspberry Pi, runs a custom linux with an ARM CPU, instead of a "normal" x86_64 CPU, meaning installing software is often harder.)</p>
<p><a href="https://en.wikipedia.org/wiki/Gudrun">Gudrun</a> will be the successor to two previous builds of mine, <a href="https://github.com/tsbertalan/hogni">Hogni</a> and <a href="https://github.com/tsbertalan/gunnar">Gunnar</a>, all three of whom were mythologically siblings. (Also, note to self, if I want to continue this naming scheme, there's a <a href="https://en.wikipedia.org/wiki/Gudrun#Family_relations">good list at the bottom</a> of that article.)</p>
</div>
<div class="mdl-cell mdl-cell--2-col mdl-cell--hide-tablet mdl-cell--hide-phone"></div>
<div class="mdl-cell mdl-cell--2-col mdl-cell--hide-tablet mdl-cell--hide-phone"></div>
<div class="demo-content mdl-color--white mdl-shadow--4dp content mdl-color-text--grey-800 mdl-cell mdl-cell--8-col">
<div class="entry-titles">
<a href="javascript:;" id="view-source" onclick="show('expand_0')" class="mdl-button mdl-js-button mdl-js-ripple-effect">Show/Hide</a><header class="tomsb-entry-header"><h2>Simple ultrasound reactive control</h2>
<p>26 February 2019</p></header>
</div>
<div id="expand_0">
<span></span><h4>Ultrasound for reactive obstacle avoidance</h4>
<p>I added two ultrasound sensors looking off to the left and right 
on the front bumper. An arduino dumps these into a serial terminal at a minimum
of about 5Hz, usually faster), and a ROS node reads these and puts them into two
<a href="http://wiki.ros.org/sensor_msgs"><code>sensor_msgs</code></a><code>.msg.Range</code> topics.</p>
<p>I implemented two behaviors--a back-up-and-turn, 
where we try to go in the direction where there's more space,
and a go-forward, where the steering fraction \(\in(-1,1)\)
is computed like
$$\tanh\left( (d_r - d_l) \cdot \lambda \right)$$
and this noisy output is passed through a 10-entry rolling mean filter
(using <a href="https://docs.python.org/2/library/collections.html#collections.deque"><code>collections.deque</code></a>!).</p>
<p>The result is an illusion of path planning! But it's still really just reactive.</p>
<p><img alt="first video" src="1_small.gif">
<img alt="second video" src="2_small.gif"></p>
<h4>In other news</h4>
<p>I've tested the <a href="http://wiki.ros.org/openni_launch"><code>openni_*</code></a> ROS packages, and found that they produce a surfeit
of depth-camera topics from my first-generation Kinect sensor (the power cable
of which I lopped off and replaced with a barrel connector to my pre-ATX 12V rail).
No accelerometer data, though--it would be nice not to have to add a separate IMU,
and instead just use the one that's in the Kinect. 
I think the <a href="http://wiki.ros.org/kinect_aux"><code>kinect_aux</code></a> package will get this for me.</p>
<p>I hope I can fake "odometry" from this IMU data,
and so obviate the need for separate wheel encoders.
However, if I need them, my current plan is to glue half a dozen tiny magnets
regularly spaced around the inside of the back wheels,
and position a <a href="https://www.sparkfun.com/products/14709">Hall-effect sensor</a>
nearby.
But I'd rather not have to make another mini-project out of getting that little
Arduino-project working properly, reading my poor-man's grey code.
Integrating an IMU in (non-embedded) software would be easier.</p>
<p>As for real planning, I still have some reading to do 
to figure out what's available already-written
for Ackermann-kinematics robots.
I've seen <a href="http://wiki.ros.org/teb_local_planner">TEB local planner</a>
used by others; I'm not sure that this would work with the same global planners
used in the gmapping stack, since some maneuvers, like N-point turns,
are fundamentally different between Ackermann and differential-drive kinematics.
I'm not above writing my own planning software
(actually, writing a quick and dirty pair of MPC-local + tree-based-global
would be a worthwhile endeavor,
and maybe not <em>too</em> much harder than getting existing packages installed,
tuned, and working smoothly),
but first I need at least write a motor controller
that react to <a href="http://wiki.ros.org/ackermann_msgs"><code>ackermann_msgs</code></a><code>.msgs.AckermannDrive</code> messages properly.</p>
<p>And, of course, getting SLAM working with the Kinect 
is a whole separate miniproject.</p>
</div>
</div>
<div class="mdl-cell mdl-cell--2-col mdl-cell--hide-tablet mdl-cell--hide-phone"></div>
<div class="mdl-cell mdl-cell--2-col mdl-cell--hide-tablet mdl-cell--hide-phone"></div>
<div class="demo-content mdl-color--white mdl-shadow--4dp content mdl-color-text--grey-800 mdl-cell mdl-cell--8-col">
<div class="entry-titles">
<a href="javascript:;" id="view-source" onclick="show('expand_1')" class="mdl-button mdl-js-button mdl-js-ripple-effect">Show/Hide</a><header class="tomsb-entry-header"><h2>Architectural plan</h2>
<p>28 February 2019</p></header>
</div>
<div id="expand_1">
<span></span><p>I've done a fair bit of thinking on where I think I should go with this project, and so I'm gathering here my thoughts on what techniques I plan to use in each level of the stack.</p>
<h4>Drivers</h4>
<p>First, I need to be sure that I'm getting all the data I'll need for subsequent steps. This will be the easiest step, and is already mostly done. I want to be able to teleop the car using only these methods, and watching visualizations from these topics, before I proceed.</p>
<ol>
<li>Respond to <a href="http://docs.ros.org/api/ackermann_msgs/html/msg/AckermannDrive.html"><code>AckermannDrive</code> messages</a>, minimally responding to the <code>steering_angle</code> and <code>speed</code> fields. Since the <a href="https://www.pololu.com/product/1351">Pololu Micro Maestro</a> servo controller I'm using to communicate with the car's ESC actually supports ramps with specified speed, a possible stretch goal would be to do something intelligent with the <code>steering_angle_velocity</code>, <code>acceleration</code>, and <code>jerk</code> fields.
   For now, it will be enough to call Pololu's provided <code>UscCmd</code> program with <code>os.system</code>  in our ROS node to set the speed and angle, but their serial protocol is <a href="https://www.pololu.com/docs/0J40/5.c">pretty well documented</a> (summarized for my use <a href="https://github.com/tsbertalan/gudrun/commit/d26840303cfa8fac44f7768aadbfb18fda8f496b">here</a>) so I could avoid the repeated subprocess by sending commands by serial directly.</li>
<li>Use the <a href="http://wiki.ros.org/openni_camera"><code>openni_*</code></a> packages to publish RGBD information. (tested and working)</li>
<li>Use <a href="http://wiki.ros.org/kinect_aux"><code>kinect_aux</code></a> to publish the Kinect's IMU information as a <code>sensor_msgs/Imu</code> topic. (not yet tested)</li>
</ol>
<h4>Perception, sensor fusion, and mapping</h4>
<p>When I have some confidence that I as a human can drive the car using the same sensor data and command topics that I'll be providing to higher-level packages, I can turn to replacing my intuition-based sensor fusion to something more concrete.</p>
<ol>
<li>Use something to fake odometry from the Kinect's IMU data. I think that this may be possible via one of the nodes in <a href="http://docs.ros.org/melodic/api/robot_localization/html/index.html#"><code>robot_localization</code></a>. This is the part I'm most uncertain about. However, since this is superficially a pretty simple task (some interesting notes <a href="http://www.seattlerobotics.org/encoder/200610/Article3/IMU%20Odometry,%20by%20David%20Anderson.htm">here</a>; it might be worthwhile to include our actual motor commands in this reckoning, but for that I'll have to sit down with pencil and paper and work out some (E/U)KF scheme), I might just write my own code to make a best-effort IMU odometer. I've seen gmapping fix some pretty egregious wheel-encoder odometry errors in the past, so I think this will be just <em>fiiiine</em>.</li>
<li>There seem to be many libraries available that will perform SLAM on RGBD or 3D point cloud data. However, after searching for a couple hours, I'm actually having difficulty finding one that will install on ROS Melodic. So, I think I'll take the simpler approach of throwing away most of the RGBD data and instead using one slice as a laserscan via the melodic-available <a href="http://wiki.ros.org/depthimage_to_laserscan"><code>depthimage_to_laserscan</code></a>.
   So, I'll probably  just use this and gmapping. KISS.</li>
</ol>
<h4>Speed control</h4>
<p>I have some doubts here. In <a href="http://tomsb.net/Gunnar/">previous work</a>, I had a tight control loop running on an Arduino to maintain commanded motor speeds. Here, I don't have wheel encoders, so the best I might be able to do is have a loose loop between my (likely very poor) odometry from the perception phase to my motor command interface in the driver phase.</p>
<p>The more I think about this, the more I think that adding some sort of wheel encoders would greatly simplify many other parts of the design. Maybe I should do that Hall-effect sensor side project after all.</p>
<h4>Planning</h4>
<p>Once I can watch the map being generated as I teleop around, I can set up a medium-level planning stack.</p>
<ol>
<li>
<a href="http://wiki.ros.org/teb_local_planner"><code>teb_local_planner</code></a> seems to be the way to go for what they call a local planner. However, the videos there suggest that this planner is capable of fairly advanced maneuvering, with multi-point turns, and it seems that it does consider multiple topologically distinct local plans. </li>
<li>However, TEB does require a <code>nav_msgs/Path</code> global plan, which can be create simply with <a href="http://wiki.ros.org/global_planner?distro=melodic#Published_Topics"><code>global_planner</code></a> (that is, A* or Djikstra, or some clever smoothed, interpolated combination) from the standard <a href="http://wiki.ros.org/navigation">navigation stack</a>.</li>
</ol>
<h4>Behaviors!</h4>
<p>This is the more fun, conceptual part. As suggested by the TEB docs, I'll need to disable some of the low-leve behaviors that come with the navigation stack (like the spin and clear). Witness instead the simple <a href="https://github.com/tsbertalan/gudrun/blob/a8cc65c6957c3498b119dbe4b59c7455c75a2977/src/gudrun_motor/ultrasound_bump_drive.py#L53"><code>Behavior</code> superclass</a> I made, without much thought, for the little bump-drive script I made last week. But, really, this section is more about high-level behaviors. I have a few ideas, some of which require more hardware/software additions than others:</p>
<p><strong>Wander around, and identify and catalog objects.</strong> Fairly easy:</p>
<ol>
<li>
<p>Set random navigation objectives (perhaps with some <a href="http://wiki.ros.org/frontier_exploration">frontier exploration</a> strategy, though that's pretty optional).</p>
</li>
<li>
<p>Wander around, take photographs annotated with current poses (and therefore, ideally, the photographed objects position--the RGBD might help a lot with this).</p>
</li>
<li>
<p>Pass them to some pre-trained object recognition neural network (I have a <a href="https://coral.withgoogle.com/">Google Edge TPU</a> I want to try to use for this), and note any high-confidence hits.</p>
</li>
<li>
<p>Assemble a database from these and make a nice frontend for querying it. Maybe a visual menu, and and for each item, there'd be a "take me there" button. The robot would drive to the remembered pose (including orientation), and then (stretch goal!), use a pair of servos to direct a laser pointer at the remembered position of the object (or even redetect it live and point to the object's updated location).</p>
</li>
</ol>
<p><strong>Patrol.</strong> In addition to or in place of the previous goal, we'd explore as far as possible (e.g., a whole apartment), and then continuously revisit the areas we saw the least recently. A nice trick would be to do some sort of anomaly detection. Somehow featureize all the views of the apartment (probably just camera+pose) and then continuously do some unsupervised learning to detect when these features go away from the typical. Obviously, the border of the "typical" region in feature space will become better-characterized as we gather more data, and there will be lots of false positives at first (it's dark! this is unfamiliar! I'm afraid!).</p>
<p><strong>Recharge.</strong> In addition to either or both of the previous, it would be great if Gudrun could recharge herself. I have had some thoughts on this, but put it on hold as overcomplicated and not necessary for now. Basically, though, my thoughts are divided between</p>
<ol>
<li>
<p>loading onto the underdeck a pair of simple NiMH and Li-Ion (balancing) battery chargers designed for standalone use, and then using a complex arrangement of transistors, relays, and ADC voltage sensing to flip power over suddenly from battery to an external bumper; and</p>
</li>
<li>
<p>wrapping the batteries in some <a href="https://hobbyking.com/en_us/6s-li-ion-10a-pcm.html">battery management system</a>, (preferably with balancing capability), and trying to avoid letting the batteries ever get so low that they need a proper "charger". Basically, float-charging them at below nominal voltage. I have a couple of ~5V solar panels that might contribute here, especially if I do ...</p>
</li>
</ol>
<p><strong>Outdoor path following.</strong> Here, the more focused goals of above might be discarded in place of just being able to navigate reliably across campus. However, an issue with this is that the view distances are typically <em>much</em> longer than in indoor scenes, and so path planning is very different. However, in a way, there's more opportunity for creativity here, since it's less geometric and special-senor based, and more computer vision. Really, though, since this would involve significant changes to the above stack, it would almost be separate project. But, as the weather gets warmer, this might beckon more strongly...</p>
</div>
</div>
<div class="mdl-cell mdl-cell--2-col mdl-cell--hide-tablet mdl-cell--hide-phone"></div>
<div class="mdl-cell mdl-cell--2-col mdl-cell--hide-tablet mdl-cell--hide-phone"></div>
<div class="demo-content mdl-color--white mdl-shadow--4dp content mdl-color-text--grey-800 mdl-cell mdl-cell--8-col">
<div class="entry-titles">
<a href="javascript:;" id="view-source" onclick="show('expand_2')" class="mdl-button mdl-js-button mdl-js-ripple-effect">Show/Hide</a><header class="tomsb-entry-header"><h2>Open-loop control of speed</h2>
<p>2 March 2019</p></header>
</div>
<div id="expand_2">
<span></span><h2>Gathering data</h2>
<p>I expect that, eventually, I will need closed-loop control of speed. So, I've ordered some <a href="https://www.amazon.com/gp/product/B07BHF3X86/ref=ppx_yo_dt_b_asin_title_o00_s00?ie=UTF8&amp;psc=1">tiny 2 mm cylindrical magnets</a> and <a href="https://www.sparkfun.com/products/14709">Hall-effect sensors</a>, and later I'll try gluing the magnets onto the inside of my rear wheels, watching for their passage with the hall effect sensors, and getting a crude measurement of velocity from that.</p>
<p>But, for now, I want to know simply what are <em>reasonable</em> throttle values to use for a given requested velocity. This is a crude form of open-loop control: give the control action that your internal model says <em>should</em> be right under the current circumstances to achieve the desired process output.</p>
<p>At first, I dutifully made a launchfile that drove forward at a specified throttle for a couple seconds, and started measuring driven distances with my tape measure. However, I quickly realized that I could get a lot more data at higher quality if I used my ultrasound, so I wrote a <a href="https://github.com/tsbertalan/gudrun/blob/82daeba2927c37f7dfff13ec2c8df2948180a413/src/gudrun_motor/launch/speed_record.launch">different launch file</a> that also started the ultrasound, and recorded time, throttle, steering, and ultrasound measurements to a <a href="https://github.com/tsbertalan/gudrun/blob/82daeba2927c37f7dfff13ec2c8df2948180a413/src/gudrun_motor/speed_record_data/speed_record.csv">CSV file</a>.</p>
<p>I then loaded this data into a <a href="https://github.com/tsbertalan/gudrun/blob/82daeba2927c37f7dfff13ec2c8df2948180a413/src/gudrun_motor/speed_record_data/Speed%20Record%20Data.ipynb">Jupyter notebook</a>. It quickly became clear that I could split up my experiments by looking for big spikes in the \(\Delta t\) between ultrasound measurements.</p>
<p>And then, for each experiment, there were some clear outliers--caused, I imagine, by the ultrasound picking up some spurious echoes. I rejected these in my regression by using <a href="https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.RANSACRegressor.html">Scikit-learn's RANSAC linear regressor</a>, and tuning the <code>residual_threshold</code> parameter to reject what, to my eye, seemed appropriate for rejection in some spot checks.</p>
<p><img src="sample_experiment.png" width="100%/"></p>
<p>In the experiment shown above, you can see that our distance to the wall decreases steadily as our fixed throttle maintains fixed speed (in equilibrium against the rolling resistance, on flat ground), until we eventually hit the wall (or, more accurately, we first hit the <a href="https://github.com/tsbertalan/gudrun/blob/82daeba2927c37f7dfff13ec2c8df2948180a413/src/gudrun_sensors/listen_to_ultrasound.py#L34"><code>min_range</code></a> for our ultrasound topic), leading to some outliers at the right side of the plot (around \(t=1086\) seconds)</p>
<p>The important thing to get from this experiment at a throttle value of 0.24 was the slope of this line--the centimeters per second value. I managed to get 23 other such pairs from the data, and created a calibration curve.</p>
<p><img src="throttle_rates.png" width="100%/"></p>
<p>Here, I again did two more regressions, and, again, I used RANSAC to throw away one obvious outliers. I probably could have just done this manually, but I might want to get more data in the future&#8212;especially in the forward direction--and this approach generalizes better. One important thing to notice here is that there is a significant dead zone in the throttles. I could perhaps characterize this better by gathering some more data in the approach to and beyond the transition into the dead zone, especially in the forward direction.</p>
<p>Another thing that might be worth doing is dumping the ultrasound for this and instead using my fancy depth camera--this will be less susceptible to the echo problem of the ultrasound (which got worse as I got further from the wall). However, with the RANSAC to reject outliers, it may be possible to get significantly further from the wall than the 1.2 meters or so that I was doing in the above batch of experiments. This is important because it would enable taking data at higher speeds. </p>
<p>(Here the point where I admit that, at one point, I disabled my <a href="https://github.com/tsbertalan/gudrun/blob/82daeba2927c37f7dfff13ec2c8df2948180a413/src/gudrun_motor/speed_record.py#L27"><code>MAX_THROTTLE_ABS</code></a> safeguard, and then, due to a bad command line, ended up running into the wall at max throttle, with about 15 feet of runway to get up to speed. Luckily, I had a spare <a href="https://www.sparkfun.com/products/13959">ultrasound sensor</a> to replace the one that seemed to stop working at this point, and I was back online in minutes.)</p>
</div>
</div>
<div class="mdl-cell mdl-cell--2-col mdl-cell--hide-tablet mdl-cell--hide-phone"></div>
</div></main>
</div></body>
</html>
